# -*- coding: utf-8 -*-
"""testaudioanalyzer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DmByCEPKzmTIIGSgFlBdwkHFBQK_NmiX
"""

!pip install noisereduce
import pandas as pd
import librosa
import soundfile as sf
import os
import noisereduce as nr
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
import kagglehub
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the dataset and process it
dataset_path = kagglehub.dataset_download("uwrfkaggler/ravdess-emotional-speech-audio")

audio_path = ''
for folder in os.listdir(dataset_path):
    if os.path.isdir(os.path.join(dataset_path, folder)):
        audio_path = os.path.join(dataset_path, folder)
        break

# Preprocessing the audio data
target_sr = 16000

for file in os.listdir(audio_path):
    if file.endswith(".wav"):
        file_path = os.path.join(audio_path, file)
        try:
            audio, sr = librosa.load(file_path, sr=None)
            audio_no_noise = nr.reduce_noise(y=audio, sr=sr)

            if sr != target_sr:
                audio_resampled = librosa.resample(audio_no_noise, orig_sr=sr, target_sr=target_sr)
            else:
                audio_resampled = audio_no_noise

            processed_path = os.path.join(audio_path, f"processed_{file}")
            sf.write(processed_path, audio_resampled, target_sr)

        except Exception as e:
            print(f"Error processing {file}: {e}")

# Feature extraction
def extract_features(audio_path):
    all_features = []
    labels = []

    for file in os.listdir(audio_path):
        if file.endswith(".wav") and file.startswith("processed_"):
            file_path = os.path.join(audio_path, file)
            try:
                audio, sr = librosa.load(file_path, sr=target_sr)
                mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13), axis=1)
                zcr = np.mean(librosa.feature.zero_crossing_rate(y=audio))
                rms = np.mean(librosa.feature.rms(y=audio))
                spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sr), axis=1)
                chroma = np.mean(librosa.feature.chroma_stft(y=audio, sr=sr), axis=1)

                feature_vector = np.hstack((mfccs, zcr, rms, spectral_contrast, chroma))
                all_features.append(feature_vector)

                label = file.split("-")[2]
                labels.append(label)

            except Exception as e:
                print(f"Error extracting features from {file_path}: {e}")

    return pd.DataFrame(all_features), pd.DataFrame(labels, columns=['label'])

features_df, labels_df = extract_features(audio_path)

if features_df.empty:
    print("Error: No features were extracted.")
else:
    features_df.to_csv("features.csv", index=False)
    labels_df.to_csv("labels.csv", index=False)

    features = pd.read_csv("features.csv")
    labels = pd.read_csv("labels.csv")

    encoder = LabelEncoder()
    labels_encoded = encoder.fit_transform(labels['label'])

    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels_encoded, test_size=0.2, random_state=42)

    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    # Calculate model accuracy
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Model Accuracy: {accuracy:.2%}")

# Function for predicting emotion from an uploaded audio file
def predict_emotion(audio_file_path, model, scaler, encoder, target_sr=16000):
    try:
        audio, sr = librosa.load(audio_file_path, sr=None)
        audio_no_noise = nr.reduce_noise(y=audio, sr=sr)

        if sr != target_sr:
            audio_resampled = librosa.resample(audio_no_noise, orig_sr=sr, target_sr=target_sr)
        else:
            audio_resampled = audio_no_noise

        mfccs = np.mean(librosa.feature.mfcc(y=audio_resampled, sr=target_sr, n_mfcc=13), axis=1)
        zcr = np.mean(librosa.feature.zero_crossing_rate(y=audio_resampled))
        rms = np.mean(librosa.feature.rms(y=audio_resampled))
        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio_resampled, sr=target_sr), axis=1)
        chroma = np.mean(librosa.feature.chroma_stft(y=audio_resampled, sr=target_sr), axis=1)

        feature_vector = np.hstack((mfccs, zcr, rms, spectral_contrast, chroma))
        feature_scaled = scaler.transform([feature_vector])

        prediction = model.predict(feature_scaled)
        predicted_emotion_label = encoder.inverse_transform(prediction)[0]
        return predicted_emotion_label

    except Exception as e:
        print(f"Error predicting emotion: {e}")
        return None

# Example usage
uploaded_audio_path = "/content/New Recording 20.wav"
predicted_emotion = predict_emotion(uploaded_audio_path, model, scaler, encoder)
emotions = ["Angry", "Calm", "Disgusted", "Fearful", "Happy", "Neutral", "Sad", "Surprised"]
print(f"Predicted Emotion: {emotions[predicted_emotion]}")